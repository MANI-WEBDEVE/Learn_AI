{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "SDt5vhNHnKvN",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:47:43.574834900Z",
     "start_time": "2025-08-23T23:46:19.146992Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "data=pd.read_csv('sms_spam_dataset.csv', encoding=\"latin-1\")"
   ],
   "metadata": {
    "id": "ehdetI_lnXjL",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:46:32.903405Z",
     "start_time": "2025-08-23T23:46:21.943011Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "data=data[['v1', 'v2']]\n",
    "data.columns=['label', 'message']\n"
   ],
   "metadata": {
    "id": "_yGT8xkWnzqX",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:46:36.042502Z",
     "start_time": "2025-08-23T23:46:35.840464Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "data.head()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pTU1t6b-n0dv",
    "outputId": "380c70a8-9066-4dd0-c4f9-dbc50cb0bcba",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:46:38.086173Z",
     "start_time": "2025-08-23T23:46:36.090468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "data['label']=data['label'].map({'ham':0, 'spam':1})"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EfBrAeHnpmfI",
    "outputId": "f6affa91-7d4c-4d0b-c68e-768df2bbd862",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:46:39.139313Z",
     "start_time": "2025-08-23T23:46:38.899313Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "PDYus2F5pviA",
    "outputId": "878bff26-39b9-4472-b7e7-f68d8a84c876",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:46:39.469846Z",
     "start_time": "2025-08-23T23:46:39.266314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   label                                            message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "print(data['label'].value_counts())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJnJi-Ezpt-X",
    "outputId": "9e5271fb-2aae-4c06-f8e8-0ca578a2d5a0",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:46:42.753511Z",
     "start_time": "2025-08-23T23:46:39.876837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    4825\n",
      "1     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "x_train , x_test, y_train, y_test=train_test_split(data['message'], data['label'], test_size=0.2, random_state=42, stratify=data['label'])",
   "metadata": {
    "id": "teCBY7RHp5yH",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:54:07.159244Z",
     "start_time": "2025-08-23T23:54:06.714692Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "vect = CountVectorizer(stop_words='english', lowercase=True)\n",
    "x_train=vect.fit_transform(x_train)\n",
    "x_test=vect.transform(x_test)"
   ],
   "metadata": {
    "id": "JP9DgFihqsB_",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:47:06.093427Z",
     "start_time": "2025-08-23T23:47:05.314898Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "mnb=MultinomialNB(alpha=0.2)\n",
    "mnb.fit(x_train, y_train)\n",
    "\n",
    "# predict the test data\n",
    "y_predmnb=mnb.predict(x_test)\n",
    "y_probmnb=mnb.predict_proba(x_test)\n",
    "\n",
    "print(y_predmnb)\n",
    "print(y_test)\n",
    "print(y_probmnb[:, 1])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "e151OBvCrT7X",
    "outputId": "d40cc9cd-6cf7-4e20-d1b7-c77a601aa258",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:47:06.880629Z",
     "start_time": "2025-08-23T23:47:06.769786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "2826    0\n",
      "3695    0\n",
      "3906    0\n",
      "575     1\n",
      "2899    0\n",
      "       ..\n",
      "854     0\n",
      "5044    0\n",
      "2015    0\n",
      "3381    0\n",
      "785     0\n",
      "Name: label, Length: 1115, dtype: int64\n",
      "[3.38404113e-16 1.44932912e-04 7.58815896e-06 ... 1.57197516e-08\n",
      " 5.33993650e-04 1.44138979e-04]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "print(accuracy_score(y_test, y_predmnb))\n",
    "print(classification_report(y_test, y_predmnb, target_names=['Ham', 'spam']))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zSnSzP6lsVR5",
    "outputId": "3f63a7a5-3cbe-4ca2-ca75-701605948471",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:47:07.158614Z",
     "start_time": "2025-08-23T23:47:06.997619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9874439461883409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.99      1.00      0.99       966\n",
      "        spam       0.97      0.93      0.95       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.96      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Small data set for SMS span detection"
   ],
   "metadata": {
    "id": "5ISo66DTwrIQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Training Data\n",
    "X_train = [\n",
    "    \"free money now\",\n",
    "    \"win free prize today\",\n",
    "    \"hey meeting tomorrow\",\n",
    "    \"free meeting at noon\",\n",
    "    \"congratulations you won thousand\"\n",
    "]\n",
    "y_train = [\"spam\", \"spam\", \"ham\", \"ham\", \"spam\"]\n",
    "\n",
    "# Vectorizer (Bag of Words with Laplace smoothing handled internally)\n",
    "vectorizer = CountVectorizer()\n",
    "X_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Train Multinomial NB\n",
    "clf = MultinomialNB(alpha=1)  # alpha=1 → Laplace smoothing\n",
    "clf.fit(X_vec, y_train)\n",
    "\n",
    "# Test\n",
    "test_msg = [\"join meeting today\"]\n",
    "test_vec = vectorizer.transform(test_msg)\n",
    "\n",
    "# Predict\n",
    "pred = clf.predict(test_vec)\n",
    "proba = clf.predict_proba(test_vec)\n",
    "\n",
    "print(\"Prediction:\", pred)\n",
    "print(\"Probabilities (ham, spam):\", proba)\n",
    "\n",
    "# Feature names (words)\n",
    "print(\"\\nVocabulary:\")\n",
    "for word, idx in vectorizer.vocabulary_.items():\n",
    "    print(f\"{word} → {idx}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRin9O_OwwQP",
    "outputId": "13d52694-0758-4430-b55a-6541d42ec2ba",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:47:07.400143Z",
     "start_time": "2025-08-23T23:47:07.244614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['ham']\n",
      "Probabilities (ham, spam): [[0.58275862 0.41724138]]\n",
      "\n",
      "Vocabulary:\n",
      "free → 2\n",
      "money → 5\n",
      "now → 7\n",
      "win → 12\n",
      "prize → 8\n",
      "today → 10\n",
      "hey → 3\n",
      "meeting → 4\n",
      "tomorrow → 11\n",
      "at → 0\n",
      "noon → 6\n",
      "congratulations → 1\n",
      "you → 14\n",
      "won → 13\n",
      "thousand → 9\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparision MNB and LogR"
   ],
   "metadata": {
    "id": "dH5To-MLume0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Plot calibration\n",
    "prob_nb = mnb.predict_proba(x_test)[:,1]\n",
    "prob_lr = LogisticRegression().fit(x_train, y_train).predict_proba(x_test)[:,1]\n",
    "\n",
    "fop_nb, mpv_nb = calibration_curve(y_test, prob_nb, n_bins=10)\n",
    "fop_lr, mpv_lr = calibration_curve(y_test, prob_lr, n_bins=10)\n",
    "\n",
    "print(fop_nb)\n",
    "print(mpv_nb)\n",
    "\n",
    "plt.plot(mpv_nb, fop_nb, 's-', label='Naive Bayes')\n",
    "plt.plot(mpv_lr, fop_lr, 'o-', label='Logistic Regression')\n",
    "plt.plot([0,1], [0,1], '--', label='Perfect')\n",
    "plt.legend()\n",
    "plt.title(\"Calibration: NB vs LR\")\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "XeOyN7jds-3Y",
    "outputId": "ac6fc358-ebec-4da0-99e9-9e962fdb267f",
    "ExecuteTime": {
     "end_time": "2025-08-23T23:57:35.176560Z",
     "start_time": "2025-08-23T23:57:33.705121Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'K k :-):-) then watch some films.'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Plot calibration\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m prob_nb = \u001B[43mmnb\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_test\u001B[49m\u001B[43m)\u001B[49m[:,\u001B[32m1\u001B[39m]\n\u001B[32m      3\u001B[39m prob_lr = LogisticRegression()\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# fop_nb, mpv_nb = calibration_curve(y_test, prob_nb, n_bins=10)\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# fop_lr, mpv_lr = calibration_curve(y_test, prob_lr, n_bins=10)\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# plt.title(\"Calibration: NB vs LR\")\u001B[39;00m\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# plt.show()\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\anaconda_setup\\envs\\venv\\Lib\\site-packages\\sklearn\\naive_bayes.py:148\u001B[39m, in \u001B[36m_BaseNB.predict_proba\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m    132\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpredict_proba\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[32m    133\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    134\u001B[39m \u001B[33;03m    Return probability estimates for the test vector X.\u001B[39;00m\n\u001B[32m    135\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    146\u001B[39m \u001B[33;03m        order, as they appear in the attribute :term:`classes_`.\u001B[39;00m\n\u001B[32m    147\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m148\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m np.exp(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpredict_log_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\anaconda_setup\\envs\\venv\\Lib\\site-packages\\sklearn\\naive_bayes.py:126\u001B[39m, in \u001B[36m_BaseNB.predict_log_proba\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m    110\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[33;03mReturn log-probability estimates for the test vector X.\u001B[39;00m\n\u001B[32m    112\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    123\u001B[39m \u001B[33;03m    order, as they appear in the attribute :term:`classes_`.\u001B[39;00m\n\u001B[32m    124\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    125\u001B[39m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m X = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_check_X\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    127\u001B[39m jll = \u001B[38;5;28mself\u001B[39m._joint_log_likelihood(X)\n\u001B[32m    128\u001B[39m \u001B[38;5;66;03m# normalize by P(x) = P(f_1, ..., f_n)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\anaconda_setup\\envs\\venv\\Lib\\site-packages\\sklearn\\naive_bayes.py:577\u001B[39m, in \u001B[36m_BaseDiscreteNB._check_X\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m    575\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_check_X\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[32m    576\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m577\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mvalidate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcsr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\anaconda_setup\\envs\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001B[39m, in \u001B[36mvalidate_data\u001B[39m\u001B[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001B[39m\n\u001B[32m   2942\u001B[39m         out = X, y\n\u001B[32m   2943\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[32m-> \u001B[39m\u001B[32m2944\u001B[39m     out = \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mX\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2945\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[32m   2946\u001B[39m     out = _check_y(y, **check_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\anaconda_setup\\envs\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001B[39m, in \u001B[36mcheck_array\u001B[39m\u001B[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[39m\n\u001B[32m   1053\u001B[39m         array = xp.astype(array, dtype, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1054\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1055\u001B[39m         array = \u001B[43m_asarray_with_order\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[43m=\u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mxp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1056\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ComplexWarning \u001B[38;5;28;01mas\u001B[39;00m complex_warning:\n\u001B[32m   1057\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1058\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mComplex data not supported\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.format(array)\n\u001B[32m   1059\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcomplex_warning\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\anaconda_setup\\envs\\venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001B[39m, in \u001B[36m_asarray_with_order\u001B[39m\u001B[34m(array, dtype, order, copy, xp, device)\u001B[39m\n\u001B[32m    837\u001B[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001B[32m    838\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m839\u001B[39m     array = \u001B[43mnumpy\u001B[49m\u001B[43m.\u001B[49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[43m=\u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    841\u001B[39m \u001B[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001B[39;00m\n\u001B[32m    842\u001B[39m \u001B[38;5;66;03m# container that is consistent with the input's namespace.\u001B[39;00m\n\u001B[32m    843\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m xp.asarray(array)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\anaconda_setup\\envs\\venv\\Lib\\site-packages\\pandas\\core\\series.py:1031\u001B[39m, in \u001B[36mSeries.__array__\u001B[39m\u001B[34m(self, dtype, copy)\u001B[39m\n\u001B[32m    981\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    982\u001B[39m \u001B[33;03mReturn the values as a NumPy array.\u001B[39;00m\n\u001B[32m    983\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   1028\u001B[39m \u001B[33;03m      dtype='datetime64[ns]')\u001B[39;00m\n\u001B[32m   1029\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1030\u001B[39m values = \u001B[38;5;28mself\u001B[39m._values\n\u001B[32m-> \u001B[39m\u001B[32m1031\u001B[39m arr = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1032\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m using_copy_on_write() \u001B[38;5;129;01mand\u001B[39;00m astype_is_view(values.dtype, arr.dtype):\n\u001B[32m   1033\u001B[39m     arr = arr.view()\n",
      "\u001B[31mValueError\u001B[39m: could not convert string to float: 'K k :-):-) then watch some films.'"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "\n",
    "calibration_curve ya method kiya kaam karata hai minglish mai samjhao\n",
    "Sure, I can explain what the calibration_curve method does in Minglish.\n",
    "\n",
    "Basically, calibration_curve is a function in scikit-learn that helps you check how \"calibrated\" your model's predicted probabilities are.\n",
    "\n",
    "Think of it like this: jab aapka model predict karta hai ki koi cheez hone ka probability 80% hai, toh kya sach mein woh cheez 80% time hoti hai? calibration_curve yeh check karta hai.\n",
    "\n",
    "It plots two things:\n",
    "\n",
    "Mean Predicted Value (MPV): Yeh aapke model ki average predicted probability hai for different \"bins\" (groups) of predictions.\n",
    "Fraction of Positives (FoP): Yeh uss bin mein actual positive cases ka fraction hai.\n",
    "Agar aapka model perfectly calibrated hai, toh MPV aur FoP almost same honge. Plot mein, yeh ek straight line banayega 45-degree pe (jo \"Perfect\" line hai plot mein).\n",
    "\n",
    "Agar aapka model under-calibrated hai, toh uski predicted probabilities actual probabilities se kam hongi. Plot mein, line \"Perfect\" line ke niche hogi.\n",
    "\n",
    "Agar aapka model over-calibrated hai, toh uski predicted probabilities actual probabilities se zyada hongi. Plot mein, line \"Perfect\" line ke upar hogi.\n",
    "\n",
    "Jo plot aapne generate kiya hai, usmein aap dekh sakte hain ki Logistic Regression ki line \"Perfect\" line ke zyada close hai Naive Bayes ke comparison mein. Iska matlab hai ki Logistic Regression ka model Naive Bayes se zyada better calibrated hai.\n",
    "\n",
    "\n",
    "Please explain this code:\n",
    "\n",
    "You're asking about the stratify parameter in the train_test_split function.\n",
    "\n",
    "\n",
    "yes\n",
    "Okay, I'll explain the stratify parameter in the train_test_split function.\n",
    "\n",
    "The stratify parameter is used to ensure that the split of your data into training and testing sets maintains the same proportion of the target variable (in your case, the 'label' column, which indicates whether an SMS is spam or not) as the original dataset.\n",
    "\n",
    "Let's say in your original data DataFrame, 15% of the SMS messages are spam and 85% are ham. If you use train_test_split with stratify=data['label'], the resulting x_train, x_test, y_train, and y_test will also have approximately 15% spam and 85% ham.\n",
    "\n",
    "This is particularly important when you have an imbalanced dataset, like the one you have (4825 ham vs 747 spam). If you don't use stratify, there's a chance that your training or testing set could end up with a significantly different proportion of spam and ham messages than the original data. This could lead to a model that performs well on one class but poorly on the other, and its overall performance might be misleading.\n",
    "\n",
    "By using stratify, you ensure that both your training and testing sets are representative of the overall data distribution, which helps in building a more robust and reliable model.\n",
    "\n",
    "Does that explanation make sense? Would you like me to explain any other part of the code?\n",
    "\n",
    "\n",
    "only response minglish language\n",
    "Sure! Main aapko stratify parameter train_test_split function mein Minglish mein samjhaata hoon.\n",
    "\n",
    "stratify parameter ka matlab hai ki jab aap apne data ko train aur test sets mein divide karte hain, toh aap yeh ensure karte hain ki aapki target variable (iss case mein 'label' column, jo batata hai ki SMS spam hai ya nahi) ka distribution original dataset ke jaisa hi rahe.\n",
    "\n",
    "Simple terms mein, agar aapke original data mein 15% SMS spam hain aur 85% ham hain, toh stratify=data['label'] use karne se aapke x_train, x_test, y_train, aur y_test mein bhi lagbhag 15% spam aur 85% ham honge.\n",
    "\n",
    "Yeh bahut important hai, especially jab aapke paas imbalanced dataset ho, jaise aapka hai (4825 ham vs 747 spam). Agar aap stratify use nahi karte hain, toh chances hain ki aapke train ya test set mein spam aur ham ka proportion original data se alag ho sakta hai. Isse aapka model ek class pe accha perform kar sakta hai aur doosre pe nahi, aur overall performance galat ho sakti hai.\n",
    "\n",
    "stratify use karke, aap yeh ensure karte hain ki aapke train aur test sets dono data ke overall distribution ko represent karte hain, jisse aap ek zyada reliable model bana sakte hain.\n",
    "\n",
    "Kya yeh explanation aapko samajh mein aaya? Kuch aur poochhna hai?\n",
    "\n",
    "\n",
    "Please explain this code:\n",
    "\n",
    "Ji haan, main aapko CountVectorizer ke baare mein Minglish mein batata hoon.\n",
    "\n",
    "CountVectorizer ek tool hai jo text data ko machine learning models ke liye numeric data mein convert karta hai. Iska kaam yeh hai ki yeh aapke text messages mein se words (ya \"tokens\") ko count karke ek matrix banata hai.\n",
    "\n",
    "Yeh kaise kaam karta hai:\n",
    "\n",
    "Vocabulary Banata Hai: Pehle, yeh aapke saare training messages mein se saare unique words ki list banata hai. Is list ko vocabulary kehte hain.\n",
    "Matrix Banata Hai: Phir, har message ke liye, yeh count karta hai ki vocabulary ka har word uss message mein kitni baar aaya hai. Isse ek matrix banti hai jismein rows messages hote hain aur columns vocabulary ke words hote hain. Cell ki value batati hai ki woh particular word uss message mein kitni baar repeat hua hai.\n",
    "Aapke code mein:\n",
    "\n",
    "vect = CountVectorizer(stop_words='english', lowercase=True): Yahaan aap CountVectorizer ka ek object bana rahe hain.\n",
    "stop_words='english' ka matlab hai ki common English words jaise \"the\", \"a\", \"is\" ko ignore kiya jayega kyunki yeh zyada information nahi dete spam detection mein.\n",
    "lowercase=True ka matlab hai ki saare words ko lowercase mein convert kiya jayega (jaise \"Free\" aur \"free\" ko ek hi word mana jayega).\n",
    "x_train=vect.fit_transform(x_train): Yeh step sabse important hai.\n",
    "fit() method x_train ke messages ko dekhega aur vocabulary banayega.\n",
    "transform() method x_train ke messages ko uss vocabulary ke hisaab se numeric matrix mein convert karega.\n",
    "x_test=vect.transform(x_test): Yahaan aap x_test ke messages ko numeric matrix mein convert kar rahe hain, lekin yeh naye words se vocabulary ko update nahi karega. Yeh wohi vocabulary use karega jo x_train se bani thi.\n",
    "Is matrix ko phir machine learning models (jaise Naive Bayes) use karte hain spam ko detect karne ke liye.\n",
    "\n",
    "Koi aur sawal hai?\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "id": "FaaLIpcWv0Y_"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
